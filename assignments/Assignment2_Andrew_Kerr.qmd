---
title: "Assignment 2 Due 10/7 at Midnight"
author: "Andrew Kerr"
format: pdf
---

::: hidden
$$
\newcommand\E{{\mathbb{E}}}
$$
:::

## Part 1: Math

In class, we have worked with "Signal plus noise Model" (equation 1.5)

$$
\begin{aligned}
\text{Model: }& x_t = 2\cos(2\pi\frac{t+15}{50}) + w_t\\
\text{Mean function: }& \mathbb{E}(x_t) = 2\cos(2\pi\frac{t+15}{50})
\end{aligned}
$$

1.  \[5 points\] The mean function is derived in Example 2.4. Describe what happens in each step of the computation \[3 points\], and provide a "math stress" rating (1 = effortless, 100 = nightmare) and 3 emojis\[2 points\]. This is personal and there is no right answer.

    | Step | Description | Math Stress | Emojis |
    |:--:|:--:|:--:|:--:|
    | 1 | We take the expected value of the model ($x_t$) | 1 | 😄🙃😢 |
    | 2 | The expected value of a constant is the constant, so $2\cos(2\pi\frac{t+15}{50})$ is left alone while the random variable $w_t$ for white noise keeps the expected value term | 1 | 😄😊😃 |
    | 3 | The expected value of $w_t$ is 0 since $w_t$ \~ $N(0, \sigma^2_w)$ , so our final value is $2\cos(2\pi\frac{t+15}{50})$ | 1 | 😏😊😄 |

2.  \[5 points\] Is the signal plus noise model stationary in the mean?

    No since it relies on time (t is in the equation)

3.  \[5 points\] Write down $\gamma_x(s,t)$, the autocovariance function of $x_t$ \[3 points\]. You may accomplish this in any way, including asking me personally in office hours or asking a classmate. Just make sure you cite the source\![2 points\]

    $\gamma_x(s,t) = cov(x_s, x_t) = E[(x_s - \mu_s)(x_t - \mu_t)]$

    $= E[(2\cos(2\pi\frac{s+15}{50} + w_s - 2\cos(2\pi\frac{s+15}{50})(2\cos(2\pi\frac{t+15}{50} + w_t - 2\cos(2\pi\frac{t+15}{50})] = E[w_sw_t] = E[w_s]E[w_t] = 0$

    Derived myself with confirmation from you!

4.  \[6 points\] Consider the model:

    $$
    y_t = x_t - 2\cos(2\pi\frac{t+15}{50}) 
    $$

    Compute the mean function of $y_t$ \[3 points\]. Is $y_t$ stationary in the mean?\[1 point\] How do you know?\[2 points\]

    $E[x_t - 2\cos(2\pi\frac{t+15}{50})] = E[x_t] - E[2\cos(2\pi\frac{t+15}{50})]$

    $= 2\cos(2\pi\frac{t+15}{50}) - 2\cos(2\pi\frac{t+15}{50}) = 0$

    Yes, $y_t$ is stationary in the mean because it does not rely on t (it is constant!).

Part 2: Code

Note: I have set the code chunks here to have `eval: false` in the code chunk. Change that to `true` so that I can run your code easily.

0.  \[5 points\] All your code runs without errors (unless that's the point), and if there is a message, explain what it means.

1.  \[5 points\] Simulate from an AR(1) process with coefficient 0.7 and 10 data points.

```{r}
#| label: part2-1
#| eval: true
#| echo: true
library(astsa)
set.seed(123)
w <- rnorm(100)
x_t <- stats::filter(
  w, # values to use
  filter = .7, # value = coef
  sides = 1, # only do past values
  method = 'recursive') # recursive tells you it is AR()
```

2.  \[6 points\] Look at the documentation for the `stats::lag` function (run `?lag` in the console). State what package the function is in and what the function does\[4 points\]. Using `k = 1` compute a lag(1) version of `x_t` that you simulated above\[2 points\].

```{r}
#| label: part2-2
#| eval: true
#| echo: true
x_t_lag1 <- lag(x_t, k = 1)
```

The lag function in the stats package creates a lagged version of the inputted data, shifted backwards a given number of observations.

3.  \[3 points\] Run the following code and compare `x_t` and `x_t_lag1`.

```{r}
#| label: part2-3
#| eval: true
#| echo: true

cbind(x_t, x_t_lag1)
```

They are the same values just shifted 1 time place apart!

4.  Make a time series plot of `x_t` and `x_t_1`. Do you notice the same features as when in the previous question?

```{r}
#| label: part2-4
#| eval: true
#| echo: true

tsplot(x_t)
tsplot(x_t_lag1)
```

Yes! They are the same plots, just shifted 1 Time apart.

5.  Run the below code. Why are the plots different? Are either particularly useful?

```{r}
#| label: part2-5
#| eval: true
#| echo: true

plot(x_t, x_t_lag1)
plot(as.vector(x_t), as.vector(x_t_lag1))
```

The plots are different because the first plot plots the number for each observation, with the value of $x_t$ on the x-axis and the value of its lagged version on the y. Meanwhile, the 2nd plot removes the NA values, so the values of $x_t$ and its lagged version are lined up again. I would say that neither of these plots are particularly useful, but the first is more useful than the 2nd.

6.  Instead of using `stats::lag`, use `dplyr::lag` to create a new version of `x_t_lag`. Repeat the code from steps 2-5. Describe how the output has changed.

```{r}
#| label: part2-6
#| echo: true
#| eval: true
x_t_lag1 <- dplyr::lag(as.vector(x_t), n = 1)
cbind(x_t, x_t_lag1)
tsplot(x_t)
tsplot(x_t_lag1)
plot(x_t, x_t_lag1)
plot(as.vector(x_t), as.vector(x_t_lag1))
```

This version of lag does not include the last observation in the lagged form of the data (it only has 9 observations now). In the time series plot this removes the last data point, and now both the plots from step 5 look the same, with the first one missing 1.

7.  Fit an intercept-free regression model between `x_t` and `x_t_lag`. Provide the value of the slope estimate and interpret the value in the context of this simulation.

```{r}
#| label: part2-7
#| echo: true
#| eval: true
linear_model <- lm(x_t ~ -1 + x_t_lag1)
linear_model
```

Slope est: 0.6389

For each increase in 1 observation, on average the value increases by 0.6389 units.

8.  \[11 points\] Plot the `acf` of `x_t`\[2 points\] and the `acf` of the residuals from the regression model\[4 points\]. Which looks more like white noise?\[2 points\] What does this tell you about the temporal structure in `x_t` vs the residuals from the regression of `x_t` on the lag 1 version of itself?\[3 points\]

```{r}
#| label: part2-8
#| echo: true
#| eval: true
acf(x_t)
acf(linear_model$residuals)
```

The acf plot of the residuals looks more like white noise. This tells me that the linear model captures more of the temporal structure.

## Part 3: Reading

\[9 points\] Read sections [2.8](https://otexts.com/fpp3/acf.html) and [2.9](https://otexts.com/fpp3/wn.html) from Forecasting Principles and Practice. Make 3 connections \[3 points each\] to content from the course textbook (equations or similar examples.).
